{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в теорию или как связаны подгузники и пиво?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Обучение на ассоциативных правилах (далее Assocoations rules learning - ARL) представляет из себя с одной стороны, простой, с другой - довольно часто применимый в реальной жизни метод поиска взаимосвязей (ассоциаций) в датасетах или, если точнее, айтемсетах (itemsests). Именно ARL лежит в основе многих рекомендательных систем, работающих в интернет-магазинах. \n",
    "   Впервые подробно об этом заговорил Piatesky-Shapiro G [1] в работе “Discovery, Analysis, and Presentation of Strong Rules.” (1991) Более подробо тему развивали Agrawal R, Imielinski T, Swami A в работах “Mining Association Rules between Sets of Items in Large Databases” (1993) [2] и “Fast Algorithms for Mining Association Rules.” (1994) [3].\n",
    "\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "  В общем виде ARL можно опсиать как \"Who bought x also bought y\" (\"Кто купил x, также купил y\"). В основе лежит анализ транзакций (transactions), внутри каждой из которых лежит свой уникальный itemset из набора items. При помощи ARL алогритмов нахоядтся те самые \"правила\" совпаденяи items внутри одной транзакции, котоыре потом сортируются по их силе. \n",
    "\n",
    "   Да, вот все так просто и логично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   За этой простотой, однако, могут скрываться поразительные вещи, о которых common sense даже не подозревал:) \n",
    "\n",
    "   Классический случай такого когнитивного диссонанса описан в статье D.J. Power \"Ask Dan!\", опубликованной в DSSResources.com [4]: \n",
    "\n",
    "   В 1992 году, группа по консалтингу в области ритейла компании Teradata под руководством Томаса Блишока провела исследование 1.2 миллиона транзакций в 25 магазинах для ритейлера Osco Drug (нет, там продавали не наркотики и даже не лекарства, точнее, не только лекартсва. Drug Store в стране веротяного противника - формат разнокалиберных магазинов у дома). \n",
    "   После анализа всех этих транзакций самым сильным правилом получилось \"Между 17:00 и 19:00 чаще всего пиво и подгузники покупают вместе\". \n",
    "   К сожалению такое правило показалось руководству Osco Drug настолкьо контринтуитивным, что ставить подгузники на полках рядом с пивом они не стали:) Хотя объяснение паре пиво-подгузники вполне себе нашлось: когда оба члена молодой семьи возвращались с рабоыт домой (как раз часам к 5 вечера), жены обычно отправляли мужей за подгузниками в ближайшей магазин. И мужья, не долго думая, совмещали приятное с полезным - покупали подгузники по заданию жены и пиво для собственного вечернего времяпрепровождения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание Association rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак мы выяснили, что пиво и подгузники - хороший джентельменский набор, а что дальше? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас имеется некий датасет (или коллекция) D, такой, что D = {d_0 ... d_j}, где d - уникальная транзакция-itemset (например, кассовый чек).\n",
    "Внутри каждой d представлен набор items (i - item), причем в идеально случае он представлен в бинарном виде: d1 = [{Пиво: 1}, {Вода: 0}, {Кола: 1}, {...}], d2 = [{Пиво : 0}, {Вода: 1}, {Кола : 1}, {...}]. Принято каждый itemset описывать через количество ненулевых значений (k-itemset), например, [{Пиво: 1}, {Вода: 0}, {Кола: 1}] являестя 2-itemset.\n",
    "\n",
    "Если в изначальном виде не представлен, можно при желании руками его преобразовать (OHE и pd.get_dummies вам в помощь).\n",
    "\n",
    "Таким образом, датасет представляет собой разреженную матрицу со значениями {1,0}. Это будет бинарный датасет. Существуют и другие виды записи - вертикальный датасет и транзакционный датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОК, данные преобразовали, как найти правила?\n",
    "\n",
    "Существует целый ряд ключевых (если хотите - базовых) понятий в ARL, которые нам помогут эти правила вывести."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support (поддержка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое понятие  в ARL - support:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(X) = \\frac{\\{t\\in T;\\ X \\in t\\}}{|T|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где X - itemset, содержащий в себе i-items, а T - количество транзакций. Т.е. в общем виде это показатель \"частотности\" данного itemset во всех анализируемых транзакциях. Но это касается только X. Нам же интересен скорее вариант, когда у нас в одном itemset встречаются x1 и x2 (например).\n",
    "Ну тут тоже все просто. Пусть x1 = {Пиво}, а x2 = {Подгузники}, значит нам нужно посчитать, во скольких транзакициях втречаестя эта парочка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(x_1\\cup x_2) = \\frac{\\sigma(x_1 \\cup x_2)}{|T|}$, где $\\sigma $ - количество транзакций, содержащих $x_1$ и $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confidence (уровень доверия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ключевое понятие - confidence. Это показатель того, как часто наше правило срабатывает для всего датасета. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conf(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример: мы хотим посчитать confidence для правила \"кто покупает пиво, тот покупает и подгузники\".\n",
    "\n",
    "Для этого сначала посчитаем, какой support у правила \"покупает пиво\", потом посчитаем support у правила \"покупает пиво и подгузники\", и просто поделим одно на другое. Т.е. мы посчитаем в скольких случаях (транзакциях) срабатывает правило \"купил пиво $supp(X)$, купил подгузники и пиво $supp(X \\cup Y)$\"\n",
    "Ничего не напоминает? Байес смотрит на все это несколько недоуменно и с презрением:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/thomas-bayes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Lift (подъем или повышение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее понятие в нашем списке - lift. Грубо говоря, lift - это отношение \"зависимости\" items к их \"независимости\". Lift показывает, насколько items зависят друг от друга. Это очевидно из формулы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lift(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1) \\times supp(x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы хотим понять зависимость покупки пива и покупки подгузников. Для этого считаем support правила \"купил пиво и подгузники\" и делим его на произведение правил \"купил пиво\" и \"купил подгузники\". В случае, если lift = 1, мы говорим, что items независимы и правил совместной покупки тут нет. Если же lift > 1, то велечина, на которую lift, собственно, больше этой самой единицы, и покажет нам \"силу\" правила. Чем больше единицы, тем круче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conviction (убедительность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде Conviction - это \"частотность ошибок\" нашего правила. Т.е., например, как часто покупали пиво без подгузников и наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conv(x_1\\cup x_2) = \\frac{1 - supp(x_2)}{1 - conf(x_1 \\cup x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем результат по формуле выше ближе к 1, тем лучше. Например, если conviction покупки пива и подгузников вместе равен 1.2, это значит, что правило \"купил пиво и подгузники\" было бы в 1.2 раза (на 20%) более верным, чем если бы совпадение этих items в одной транзакции было бы чисто случайным. Немного неинтуитивное понятие, но оно и используестя н етак часто, как предыдущие три. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Существует ряд часто используемых алгоритмов, позволяющих находить правила в itemsets согласно перечисленным выше понятиям - Наивный или брутфорс-алгоритм, Apriori- алгоритм, ECLAT-алгоритм, FP-growth алгоритм и другие."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Брутфорс"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найти правила в itemsets в общем не сложно, сложно сделать это эффективно. Брутфорс-алгоритм самый простой и, в то же время, самый неэффективный способ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В псевдо-коде алгоритм выглядит так:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Наборы itemsets $F_1, F_2, F_3, ... F_q$, где $F_i$ - набор itemsets размера $I$, которые встечаются как минимум $s$ раз в $D$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $R \\leftarrow$  целочистленный array, содержащий в себе все комбинации items в $D$, размера $2^{|D|}$\n",
    "2. **for** $n \\leftarrow  [1, |D|]$ **do**:\n",
    "<br>\n",
    "$F \\leftarrow$  все возможные комбинации из $D_n$\n",
    "<br>\n",
    "Увеличить каждое значение в $R$ согласно значениям в каждом $F[]$\n",
    "5. **return** Все itemsets в $R \\geq s$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность брутфорс-алгоритма очевидна:\n",
    "Для того чтобы найти все возможные Association rules применяя брутфорс-алгоритм нам необходимо перечислить все подмножества $X$ из набора $I$ и для каждого подмножества $X$ рассчитать $supp(X)$. Данный подход будет состоять из следующих шагов:\n",
    "-  генерация **кандидатов**. Данный шаг состоит из перебора всех возможных подмножеств $X$ подмножества $I$. Данные подмножества называются **кандидатами**, поскольку каждое подмножество теоретически может являться часто встречаемым подмножеством, то множество потенциальных кандидатов будет состоять из $2^{|D|}$ элементов (здесь $|D|$ обозначается число элементов множества $I$, а $2^{|D|}$ часто называется булеаном множества $I$, то есть множество всех подмножеств $I$)\n",
    "-  расчет **support**. На данном шаге рассчитывается $supp(X)$ каждого кандидата $X$\n",
    "\n",
    "Таким образом, сложность нашего алгоритма будет: $O(|I|*|D|*2^{|I|})$, где\n",
    "<br>$O(2^{|I|})$ - количество возможных кандидатов</br>\n",
    "<br>$O(|I|*|D|)$ - сложность расчета sup(X). Поскольку для расчета  $supp(X)$ нам необходимо перебрать все элементы в $I$ в каждой транзакции $t \\in T$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В нотации O-большое нам понадобится $O(N)$ операций, где $N$ - количество itemsets.\n",
    "Таким образом, для применения брутфорс-алгоритма нам понадобится $2^i$ ячеек памяти, где $i$ - индивидуальный itemset. Т.е. для хранения и обсчета 34 items нам понадобится 128GB RAM (для 64-битных целосчисленных ячеек). \n",
    "<br>Таким образом брутфорс поможет нам в обсчете транзакций в палатке с шаурмой у вокзала, но для чего-то более серьезного он совершенно не пригоден. Реализацию в Питоне и R деалть нет смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используемые понятия:\n",
    "<br> -  Множество объектов (**itemset**): $X \\subseteq I = \\{x_1, x_2, ..., x_n\\}$</br>\n",
    "<br> -  Множество идентификаторов транзакций (**tidset**): $T = \\{t_1, t_2, ..., t_m\\}$</br>\n",
    "<br> -  Множество транзакций (**transactions**): $\\{(t,\\ X):\\ t\\in T,\\ X \\in I\\}$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем дополнительно еще несколько понятий.\n",
    "<br>Будем рассматривать дерево префиксов (prefix tree), где 2 элемента $X$ и $Y$ соединены, если $X$ является прямым подмножеством $Y$. Таким образом мы можем пронумеровать все подмножества множества $I$. Рисунок приведен ниже</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, рассмотрим Apriori алгоритм.\n",
    "<br>\n",
    "Apriori алгоритм использует следующее утверждение: если $X \\subseteq Y$, то $supp(X) \\geq supp(Y)$. Откуда следуют следующие 2 свойства:\n",
    " -  если $Y$ встречается часто, то любое подмножество $X: X \\subseteq Y$  так же встречается часто\n",
    " -  если $X$ встречается редко, то любое супермножество $Y: Y \\supseteq X$ так же встречается редко\n",
    " </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriori алгоритм по-уровнево проходит по префиксному дереву и рассчитывает частоту встрчаемости подмножеств $X$ в $D$. Таким образом, в соответствии с алгоритмом:\n",
    " -  исключаются редкие подмножества и все их супемножества\n",
    " -  рассчитывается $supp(X)$ для кадого подходящего кандидата $X$ размера $k$ на уровне $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В псевдо-код нотации Априори-алгоритм выглядит следующим образом:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, и $\\sigma$ - задаваемый пользователем порог $supp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F(D, \\sigma)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $C_1$ $\\leftarrow$ $[{i}|i \\in J]$\n",
    "2. $k \\leftarrow 1$\n",
    "3. **while** $C_k \\neq 1$ **do**:\n",
    "4. #Считаем все support для всех кандидатов\n",
    "<br>\n",
    "**for all** транзакций (tid, I) $\\in D$ **do**:\n",
    "<br>\n",
    "    **for all** кандидатов $X \\in C_k$ **do**:\n",
    "<br>\n",
    "**if** $X \\subseteq I$:\n",
    "<br>\n",
    "$X.support++$\n",
    "5. #Вытаскиваем все частые itemsets:\n",
    "<br>\n",
    "$F_k = $ {X|X.support > $\\sigma$}\n",
    "6. #Генерируем новых кандидатов\n",
    "<br>\n",
    "**for all** $X,Y \\in F_i, X[i] = Y[i]$ **for** $1 \\leq i \\leq k-1$ **and** $X[k] \\leq Y[k]$ **do**:\n",
    "<br>\n",
    "$I = X \\cup$ {$Y$|$k$|}\n",
    "<br>\n",
    "**if** $\\forall J \\subset I,|J| = k: J \\in F_k$ **then**\n",
    "<br>\n",
    "$C_k+1$ $\\leftarrow C_k+1 \\cup I$\n",
    "<br>\n",
    "$k++$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Таким образом, Apriori-адгоритм сначала ищет все единичные (содержащие 1 элемент) itemsets, удовлетворяющие заданному пользователем $supp$, затем составляет из них пары такие по принципу иерархической монотонности, т.е. если $x_1$ встречается часто и $x_2$ встречается часто, то и $[x_1, x_2]$ встречается часто."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Явным минусом такого подхода является то, что необходимо \"просканировать\" весь датасет, посчитать все $supp$ на всех уровнях breadth-first search (поиск в ширину - подробнее [тут](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D0%B2_%D1%88%D0%B8%D1%80%D0%B8%D0%BD%D1%83))\n",
    "Это также может подъесть RAM на больших датасетах, зотя и намного эффективнее брутфорса"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn import... эммм... а импортировать-то и нечего:) На данный момент модулей для ALR в sklearn нет. Ну ничего, погуглим или напишем свои, правда?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "По сети гуляет целый ряд реализаций, например [вот](https://github.com/asaini/Apriori), [вот](http://adataanalyst.com/machine-learning/apriori-algorithm-python-3-0/), и даже [вот](https://codereview.stackexchange.com/questions/38101/apriori-algorithm-using-pandas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы же на практике придерживаемся алгоритма apyori, написанного Ю Мочиузки (Yu Mochizuki). Полный код приводить не будем, желающие могут посмотреть [тут](https://github.com/ymoch/apyori) , а вот архитектуру решения и пример использования покажем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условно решение Мочизуки можно разделить на 4 части: Структура данных, Внутренние функции, API и Прикладные функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая часть модуля (Структура данных) работает с изначальным датасетом. Реализуется класс TransactionManager, методы которого объединяют транзакции в матрицу, формируют список правил-кандидатов и считают support для каждого правила. Внутренние функции дополнительно по support'у формируют списки правил и соответственно их ранжируют. API логично позволяет работать напрямую с датасетами, а Прикладные функции позволяют обрабатывать транзакции и выводить результат в читаемый вид. Никакого rocketscience.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как использовать модуль на реальном (ну, в данном случае - игрушечном) датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# подгрузим модули\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузим данные\n",
    "dataset = pd.read_csv('src/data/Market_Basket_Optimisation.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.describe #всего 7500 записей-транзакций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посомтрим на датасет\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что датасет у нас представляет разреженную матрицу, где в строках у нас набор items в каждой транзакции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#создаим из них матрицу\n",
    "transactions = []\n",
    "for i in range(0, 7501): \n",
    "    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#загружаем apriori\n",
    "import apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apriori' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'apriori' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# и обучимся правилам. Обратите внимание, что пороговые значения мы вибираем сами в зависимости от того, /\n",
    "# насколкьо \"сильные\" правила мы хотим получит\n",
    "# min_support -- минимальный support для правил (dtype = float).\n",
    "# min_confidence -- минимальное значение confidence для правил (dtype = float)\n",
    "# min_lift -- минимальный lift (dtype = float)\n",
    "# max_length -- максимальная длина itemset (вспоминаем про k-itemset)  (dtype = integer)\n",
    "\n",
    "result = list(apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# вот с визуализацией в пакете проблемы, точнее геморрой...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil, os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json #преобразовывать будем в json, используя встроенные в модуль методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for RelationRecord in result:\n",
    "    o = StringIO()\n",
    "    apyori.dump_as_json(RelationRecord, o)\n",
    "    output.append(json.loads(o.getvalue()))\n",
    "data_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# и взгялнем на итоги\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы видим:\n",
    "\n",
    "1. Пары items\n",
    "2. items_base - первый элемент пары\n",
    "3. items_add - второй (добавленный алгоритмом) элемент пары\n",
    "4. confidence - значение confidence для пары\n",
    "5. lift - значение lift для пары\n",
    "6. support - начение support для пары. При желании, по нему можно отсортировать \n",
    "\n",
    "\n",
    "Результаты логичные: эскалоп и макароны, эскалоп и сливочно-грибной соус, курица и нежирная сметана, мягкий сыр и мед и т.д. - все это вполне логичные и, главное, вкусные сочетания:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ARL тот случай,скогда R-фаги могут злорадно похихикать. В R реализована библиотека arules, где присутствует и apriori, и другие алгоритмы. Официальную доку можно посмотреть [тут](https://cran.r-project.org/web/packages/arules/arules.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрим на нее в действии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала установим ее (если еще не установили):\n",
    "<br>\n",
    "install.packages('arules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные и преобразуем их в матрицу транзакций:\n",
    "<BR>\n",
    "<br>\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv', header = FALSE)$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные:\n",
    "<BR>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выучим наши правила:\n",
    "<br>\n",
    "В общем виде фнкция вызова apriori выглядит так:  $apriori(data, parameter = NULL, appearance = NULL, control = NULL)$, где\n",
    "<br>\n",
    "\n",
    "$data$ - наш датасет\n",
    "<br>\n",
    "$paramter$ - список (list) параметров для модели: минимальные support, confidence и lift\n",
    "<br>\n",
    "$appearance$ - отвечает за отображение данных. Может принимать значения lhs, rhs, both, items, none, которые определяют положение items в output\n",
    "<br>\n",
    "$control$ - отвечает за сортировку вывода (ascending, descending, без сортировки), а также за то, отображать ли прогрессбар или нет (параметр verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'lift')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что на выходе имеем примерно те же результаты, что при использовании модуля apyori в Python:\n",
    "<br>\n",
    "\n",
    "1. {light cream}                               => {chicken}       0.004532729\n",
    "2. {pasta}                                     => {escalope}      0.005865885\n",
    "3. {pasta}                                     => {shrimp}        0.005065991\n",
    "4. {eggs,ground beef}                          => {herb & pepper} 0.004132782\n",
    "5. {whole wheat pasta}                         => {olive oil}     0.007998933\n",
    "6. {herb & pepper,spaghetti}                   => {ground beef}   0.006399147\n",
    "7. {herb & pepper,mineral water}               => {ground beef}   0.006665778\n",
    "8. {tomato sauce}                              => {ground beef}   0.005332622\n",
    "9. {mushroom cream sauce}                      => {escalope}      0.005732569\n",
    "10. {frozen vegetables,mineral water,spaghetti} => {ground beef}   0.004399413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в R apriori использовать на данный момент намного удобнее, чем в Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECLAT Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Идея алгоритма ECLAT (Equivalence CLAss Transformation) заключается в ускорении подсчета $supp(X)$. Для этого нам необходимо проиндексировать наше базу данных $D$ так, чтобы это позволило быстро рассчитывать $supp(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко заметить, что если $t(X)$ обозначает множество всех транзакций, где встречается подмножество $X$, то\n",
    "<br>$t(XY) = t(X) \\cap t(Y)$</br>\n",
    "<br>и</br>\n",
    "<br>$supp(XY) = |t(XY)|$</br>\n",
    "<br> то есть $supp(XY)$ равен кардинальности (размеру) множества $t(XY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход может быть значительно усовершенствован путем уменьшения размера промежуточных множеств идентификаторов транзакций (tidsets). А именно, мы можем хранить не все множество транзакций на промежуточном уровне, а только множество различий этих транзакций. Предположим, что\n",
    "<br>$X_a = \\{x_1, x_2,..., x_{n-1}, a\\}$</br>\n",
    "<br>$X_b = \\{x_1, x_2,..., x_{n-1}, b\\}$</br>\n",
    "<br>Тогда, мы получим: </br>\n",
    "<br>$X_{ab} = \\{x_1, x_2,..., x_{n-1}, a, b\\}$</br>\n",
    "<br>$diffset(X_{ab})$ это множество всех id транзакций, которыесодержат префикс $X_a$ но не содержат элемент $b$: </br>\n",
    "<br>$d(X_{ab}) =t(X_a)/t(X_{ab})=t(X_a)/t(X_{b})$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"/src/images/tree_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В отличие от Apriori-алгоритма, ECLAT производите поиск в глубину (DFS, [подробнее тут](https://ru.wikipedia.org/wiki/%D0%9F%D0%BE%D0%B8%D1%81%D0%BA_%D0%B2_%D0%B3%D0%BB%D1%83%D0%B1%D0%B8%D0%BD%D1%83)). Иногда его называют \"вертикальным\" (в отличие от \"горизонтального\" для Apriori)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, $\\sigma$ - задаваемый пользователем порог $supp$ и новый элемент префикс $I \\subseteq J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F[I](D, \\sigma)$ для соответсвующего префикса $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $F[i] \\leftarrow$ {}\n",
    "<br>\n",
    "2. **for** all $i \\in J$ in $D$ **do**:\n",
    "<br>\n",
    "$F[I] := F[I] \\cup$ {I $\\cup$ {i}} \n",
    "<br>\n",
    "3. #Создаем $D_i$\n",
    "<br>\n",
    "$D_i \\leftarrow$ {}\n",
    "<br>\n",
    "**for** all $j \\in J$ in $D$ таких, что $j > i$ **do:**\n",
    "<br>\n",
    "$C \\leftarrow$ покрывает ({$i$} $\\cap$ покрывает {$j$})\n",
    "<br>\n",
    "**if** $|C| \\geq \\sigma$ **then**\n",
    "<br>\n",
    "$D_i \\leftarrow D_i \\cup$ {$C$,$i$}\n",
    "<br>\n",
    "4. #DFS - рекурсия:\n",
    "<br>\n",
    "Считаем $F|I \\cup$ {$i$}| $(D_i, \\sigma)$\n",
    "<br>\n",
    "$F[I]: F[I] \\cup F[I \\cup i]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ключевым понятием для ECLAT-алгоритма является I-префикс. В началае генерируется пустое множество I, это позволяет нам на первом проходе выделить вче частотные itemsets. Затем алгоритм будет вызывать сам себя и увеличивать I на 1 на каждом шаге до тех пор, пока не будет достигнута заданная пользователем длина I. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для хранения значений используется прфеиксное дерево (trie (не tree:)), [тут подробнее](https://ru.wikipedia.org/wiki/%D0%9F%D1%80%D0%B5%D1%84%D0%B8%D0%BA%D1%81%D0%BD%D0%BE%D0%B5_%D0%B4%D0%B5%D1%80%D0%B5%D0%B2%D0%BE)). Вначале строится нулевой корень дерева (то самое пустое множество I), затем по мере прохода по itemsets алгоритм прописывает содержащиеся в каждом itesmsets items, при этом самая левая ветвь является child нулевого корня и далее вниз. При этом ветвей столько, сколкьо items встречаестя в itemsets. Такой подход позволяет записывать itemset в памяти толкьо один раз, что делает ECALT быстрее Apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<<**КАРТИНКА**>>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createC1(dataset):\n",
    "    \"Create a list of candidate item sets of size one.\"\n",
    "    c1 = []\n",
    "    for transaction in dataset:\n",
    "        for item in transaction:\n",
    "            if not [item] in c1:\n",
    "                c1.append([item])\n",
    "    c1.sort()\n",
    "    #frozenset because it will be a ket of a dictionary.\n",
    "    return map(frozenset, c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def scanD(dataset, candidates, min_support):\n",
    "    \"Returns all candidates that meets a minimum support level\"\n",
    "    sscnt = {}\n",
    "    for tid in dataset:\n",
    "        for can in candidates:\n",
    "            if can.issubset(tid):\n",
    "                sscnt.setdefault(can, 0)\n",
    "                sscnt[can] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "И вновь пользователи R ликуют, для них никаких танцев с бубном делать не надо, все по аналогии с apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запускаем библиотеку и читаем данные:\n",
    "\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv')$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрый взгляд на датасет:\n",
    "<br>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правила:\n",
    "<br>\n",
    "$rules = eclat(data = dataset, parameter = list(support = 0.003, minlen = 2))$ \n",
    "<br>\n",
    "Обратите внимание, настраиваем толкьо support и минимальную длину (k в k-itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И смотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'support')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "FP-Growth (Frequent Pattern Growth) алгоритм самый молодой из нашей троицы, впервые он описан в 2000 году в [7].\n",
    "FP-Growth предлагает радикальную вещь - отказатсья от генерации **кандидатов** (напомним, шенерация кандидатов лежит в основе Apriori и ECLAT). Теоретчиески, такой подход позволит еще больше увеличитьь скорость алгоритма и использовать еще меньше памяти."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Это достигаестя за счет хранения в памяти префиксного дерева (trie) не из комбинаций кандидатов, а из самих транзакций. \n",
    "При этом FT-Growth генерирует таблицу заголовков дял каждого item, чей $supp$ выше заданного пользователем. Эта таблица заголвоков хранит связанный спсико всех однотипных узлов префиксного дерева. Таким образом, алгоритм сочетает с ебе плюсы **BFS** за счет таблицы заголовков и **DFS** за счет построения trie. Псевдокод алгоритма схож с ECALT, за некоторыми исключениями."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВХОД**: Датасет $D$, содержащий список транзакций, $sigma$ - задаваемый пользователем порог $supp$ и префикс $I \\subseteq J$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ВЫХОД**: Список itemsets $F[I](D, sigma)$ для соответсвующего префикса $I$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ПОДХОД:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $F[i] \\leftarrow$ {}\n",
    "<br>\n",
    "2. **for** all $i \\in J$ in $D$ **do**:\n",
    "<br>\n",
    "$F[I] := F[I] \\cup$ {I $\\cup$ {i}} \n",
    "<br>\n",
    "3. #Создаем $D_i$\n",
    "<br>\n",
    "$D_i \\leftarrow$ {}\n",
    "<br> \n",
    "**$H_i \\leftarrow$ {}**\n",
    "<br>\n",
    "**for** all $j \\in J$ in $D$ таких, что $j > i$ **do:**\n",
    "<br>\n",
    "**if** $supp$ ($I \\cup$ {$i$,$j$}) $\\geq \\sigma$ **then**:\n",
    "<br>\n",
    "$H \\leftarrow H \\cup$ {$j$}\n",
    "<br>\n",
    "**for** all $(tid, X) \\in D$ при $I \\in X$ **do**:\n",
    "<br>\n",
    "$D_i \\leftarrow D_i \\cup$ ({$tid,X \\cap H$})\n",
    "<br>\n",
    "4. #DFS - рекурсия:\n",
    "<br>\n",
    "Считаем $F|I \\cup$ {$i$}| $(D_i, \\sigma)$\n",
    "<br>\n",
    "$F[I] \\leftarrow F[I] \\cup F[I \\cup i]$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализации FP-Growth в Питоне повезло не больше, чем другим ALR-алгоритмам. Стандартных библиотек под него нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо FP_Growth представлен в pyspark, смотреть тут - http://spark.apache.org/docs/2.2.0/mllib-frequent-pattern-mining.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На gitHub тоже можно найти несколкьо решений эпохи неолита, например тут - https://github.com/enaeseth/python-fp-growth и тут - https://github.com/evandempsey/fp-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потестим второй вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfpgrowth\n",
      "  Downloading pyfpgrowth-1.0.tar.gz (1.6MB)\n",
      "Building wheels for collected packages: pyfpgrowth\n",
      "  Running setup.py bdist_wheel for pyfpgrowth: started\n",
      "  Running setup.py bdist_wheel for pyfpgrowth: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\pavel.golubev\\AppData\\Local\\pip\\Cache\\wheels\\b0\\77\\ee\\b8139db041c69f769d35de3373fe03129c25ef186521e924f1\n",
      "Successfully built pyfpgrowth\n",
      "Installing collected packages: pyfpgrowth\n",
      "Successfully installed pyfpgrowth-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyfpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patterns = pyfpgrowth.find_frequent_patterns(transactions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rules = pyfpgrowth.generate_association_rules(patterns, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае R не отстает от Питона: в такой удобнйо и родной arules FP-Growth отсутствует. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В то же время, как и для Питона, реализация сущетсвует в Spark - https://spark.apache.org/docs/2.2.0/api/R/spark.fpGrowth.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключении давайте эмпирически сравним **эффектвиность** метрикой *runtime* в зависимости от плотности датасета и длин транзакций датасета. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под *плотностью* датасета подразумеваестя отношение транзакций, содержащих в себе частые items к общему количеству транзакций."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/dens_run.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика очевидно, что эффективность (чем меньше runtime, тем эффективнее) Apriori-алгоритма падает при увеличении плотности датасета."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Под *длинной транзакции* понимается количество в items в itemset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/size_run.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очевидно, что при увеличении длины транзакции Apriori также справляется гораздо хуже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def sizeof_fmt(num):\n",
    "    for x in ['','k','m','g']:\n",
    "        if num < 1000.0:\n",
    "            return \"%3.1f%s\" % (num, x)\n",
    "        num /= 1000.0\n",
    "    return \"%3.1f%s\" % (num, 't')\n",
    "\n",
    "# These paramaters can be changed to determine the type of data to generate.\n",
    "# How many baskets, or transactions\n",
    "BASKET_COUNT = 10000000\n",
    "# Maximum number of items per basket\n",
    "MAX_ITEMS_PER_BASKET = 7\n",
    "# The number of unique items\n",
    "ITEM_COUNT = 200\n",
    "# The number of frequent itemsets\n",
    "NUM_FREQUENT_ITEMSETS = 2\n",
    "# The probability of a basket containing a frequent itemset.\n",
    "PROB_FREQUENT = 0.5\n",
    "\n",
    "# Generate the data\n",
    "pop_frequent = [\"F\"+str(n) for n in range(0,MAX_ITEMS_PER_BASKET)]\n",
    "pop_regular = [\"I\"+str(n) for n in range(MAX_ITEMS_PER_BASKET,ITEM_COUNT)]\n",
    "\n",
    "\n",
    "freq_itemsets = []\n",
    "\n",
    "for i in range(NUM_FREQUENT_ITEMSETS):\n",
    "    cnt = random.randint(1,MAX_ITEMS_PER_BASKET)\n",
    "    freq_itemsets.append(random.sample(pop_frequent,cnt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filename that encodes the MAX_ITEMS_PER_BASKET and BASKET_COUNT into\n",
    "# the filename.\n",
    "with open('C:/Users/pavel.golubev/Desktop/arules_shared/src/data/fprob'+str(PROB_FREQUENT)+\"_tsz\"\n",
    "        +str(MAX_ITEMS_PER_BASKET)+'_tct'\n",
    "        +sizeof_fmt(BASKET_COUNT)+'.txt', 'w') as f:\n",
    "    for i in range(BASKET_COUNT):\n",
    "        line = []\n",
    "\n",
    "        cnt = random.randint(1,MAX_ITEMS_PER_BASKET)\n",
    "        if random.random()<=PROB_FREQUENT:\n",
    "            idx = random.randint(0,len(freq_itemsets)-1)\n",
    "            for j in range(len(freq_itemsets[idx])):\n",
    "                line.append(freq_itemsets[idx][j])\n",
    "\n",
    "        needed = max(0,cnt - len(line))\n",
    "        line = line + random.sample(pop_regular,needed)\n",
    "\n",
    "        f.write(\" \".join(line)+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Итоги"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак, мы познакомились с базовой теорией ARL (\"кто купил х, также купил y\") и основными понятиями и метриками (support, confidence, lift и conviction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрели 3 самых популярных алгоритма (Apriori, ECLAT, FP-Growth), позавидовали пользователям R и библиотеки arules, попробовали сами реализовать ECALT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные моменты:\n",
    "1. ARL лежат в основе рекомендательных систем\n",
    "2. ARL широко применимы - от традиционного ритейла и онлайн ритейла (от Ozon до Steam) до банков и телекома (подключаемые сервисы и услуги)\n",
    "3. ARL относительно легко использовать, существуют реализации разного уровня проработки для разных задач.\n",
    "4. ARL хорошо интепретируются и не требуют специальных навыков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо рассмотренных бызовых алгоритмов существет модификации и ответвления:\n",
    "\n",
    "1. AprioriDP (Deep Programming) - позволяет хранить $supp$ в специальной структруе данных, работает немного быстрее классичсекого Apriori\n",
    "2. FP Bonsai - улучшенный FP-Growth с обрезкой префиксного дерева (пример алгоритма с **ограничениями**\n",
    "3. Серия Multi-Relations алгоритмов, которые определают несколкьо свзяей в itemsets.\n",
    "4. и другие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключении не можем не упомянуть о сумрачном гении ARL докторе Кристиане Боргельте из Университета Констанца (http://www.borgelt.net/software.html).\n",
    "На самом деле все, с чем мы мучались на протяжении этой статьи им давно уже реализовано на С, Python, Java и R. Ну или почти все. Существет даже GUI за его авторством, где в пару кликов можно загрузить датасет, выбрать нужный алгоритм и найти правила. Это при условии, что оно у вас заработает:)\n",
    "Для простых же задач достаточно и того, что мы рассмотрели в этой статье. А если недостаточно - призываем писать реализацию самим! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Использованная литература:**\n",
    "<br>\n",
    "[1] Discovery, analysis and presentation of strong rules. G. Piatetsky-Shapiro. Knowledge Discovery in Databases, AAAI Press, (1991)\n",
    "<br>\n",
    "[2] Mining Association Rules between Sets of Items in Large Databases http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf\n",
    "<br>\n",
    "[3] Fast Algorithms for Mining Association Rules http://www.vldb.org/conf/1994/P487.PDF\n",
    "<br>\n",
    "[4] Ask Dan! http://www.dssresources.com/newsletters/66.php\n",
    "<br>\n",
    "[5] Introduction to arules – A computational environment for mining association rules and frequent item sets http://www.lsi.upc.edu/~belanche/Docencia/mineria/Practiques/R/arules.pdf\n",
    "<br>\n",
    "[6] Публикации Д-ра Боргельта - http://www.borgelt.net/publications.html\n",
    "<br>\n",
    "[7] J. Han, J. Pei, and Y. Yin, “Mining frequent patterns without candidate generation,” in ACM SIGMOD Record, vol. 29, no. 2. ACM, 2000, pp. 1–12.\n",
    "<br>\n",
    "[8] Shimon, Sh. Improving Data mining algorithms using constraints. The Open University of Israel, 2012."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
