{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Math, Latex\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Введение в теорию или как связаны подгузники и пиво?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   Обучение на ассоциативных правилах (далее Assocoations rules learning - ARL) представляет из себя с одной стороны, простой, с другой - довольно часто применимый в реальной жизни метод поиска взаимосвязей (ассоциаций) в датасетах или, если точнее, айтемсетах (itemsests). Именно ARL лежит в основе многих рекомендательных систем, работающих в интернет-магазинах. \n",
    "   Впервые подробно об этом заговорил Piatesky-Shapiro G [1] в работе “Discovery, Analysis, and Presentation of Strong Rules.” (1991) Более подробо тему развивали Agrawal R, Imielinski T, Swami A в работах “Mining Association Rules between Sets of Items in Large Databases” (1993) [2] и “Fast Algorithms for Mining Association Rules.” (1994) [3].\n",
    "\n",
    "    \n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   \n",
    "  В общем виде ARL можно опсиать как \"Who bought x also bought y\" (\"Кто купил x, также купил y\"). В основе лежит анализ транзакций (transactions), внутри каждой из которых лежит свой уникальный itemset из набора items. При помощи ARL алогритмов нахоядтся те самые \"правила\" совпаденяи items внутри одной транзакции, котоыре потом сортируются по их силе. \n",
    "\n",
    "   Да, вот все так просто и логично."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   За этой простотой, однако, могут скрываться поразительные вещи, о которых common sense даже не подозревал:) \n",
    "\n",
    "   Классический случай такого когнитивного диссонанса описан в статье D.J. Power \"Ask Dan!\", опубликованной в DSSResources.com [4]: \n",
    "\n",
    "   В 1992 году, группа по консалтингу в области ритейла компании Teradata под руководством Томаса Блишока провела исследование 1.2 миллиона транзакций в 25 магазинах для ритейлера Osco Drug (нет, там продавали не наркотики и даже не лекарства, точнее, не только лекартсва. Drug Store в стране веротяного противника - формат разнокалиберных магазинов у дома). \n",
    "   После анализа всех этих транзакций самым сильным правилом получилось \"Между 17:00 и 19:00 чаще всего пиво и подгузники покупают вместе\". \n",
    "   К сожалению такое правило показалось руководству Osco Drug настолкьо контринтуитивным, что ставить подгузники на полках рядом с пивом они не стали:) Хотя объяснение паре пиво-подгузники вполне себе нашлось: когда оба члена молодой семьи возвращались с рабоыт домой (как раз часам к 5 вечера), жены обычно отправляли мужей за подгузниками в ближайшей магазин. И мужья, не долго думая, совмещали приятное с полезным - покупали подгузники по заданию жены и пиво для собственного вечернего времяпрепровождения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание Association rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак мы выяснили, что пиво и подгузники - хороший джентельменский набор, а что дальше? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пусть у нас имеется некий датасет (или коллекция) D, такой, что D = {d_0 ... d_j}, где d - уникальная транзакция-itemset (например, кассовый чек).\n",
    "Внутри каждой d представлен набор items (i - item), причем в идеально случае он представлен в бинарном виде: d1 = [{Пиво: 1}, {Вода: 0}, {Кола: 1}, {...}], d2 = [{Пиво : 0}, {Вода: 1}, {Кола : 1}, {...}]. Принято каждый itemset описывать через количество ненулевых значений (k-itemset), например, [{Пиво: 1}, {Вода: 0}, {Кола: 1}] являестя 2-itemset.\n",
    "\n",
    "Если в изначальном виде не представлен, можно при желании руками его преобразовать (OHE и pd.get_dummies вам в помощь).\n",
    "\n",
    "Таким образом, датасет представляет собой разреженную матрицу со значениями {1,0}. Это будет бинарный датасет. Существуют и другие виды записи - вертикальный датасет и транзакционный датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ОК, данные преобразовали, как найти правила?\n",
    "\n",
    "Существует целый ряд ключевых (если хотите - базовых) понятий в ARL, которые нам помогут эти правила вывести."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support (поддержка)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первое понятие  в ARL - support:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(X) = \\frac{\\{t\\in T;\\ X \\in t\\}}{|T|}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ", где X - itemset, содержащий в себе i-items, а T - количество транзакций. Т.е. в общем виде это показатель \"частотности\" данного itemset во всех анализируемых транзакциях. Но это касается только X. Нам же интересен скорее вариант, когда у нас в одном itemset встречаются x1 и x2 (например).\n",
    "Ну тут тоже все просто. Пусть x1 = {Пиво}, а x2 = {Подгузники}, значит нам нужно посчитать, во скольких транзакициях втречаестя эта парочка. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$supp(x_1\\cup x_2) = \\frac{\\sigma(x_1 \\cup x_2)}{|T|}$, где $\\sigma $ - количество транзакций, содержащих $x_1$ и $x_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Confidence (уровень доверия)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее ключевое понятие - confidence. Это показатель того, как часто наше правило срабатывает для всего датасета. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conf(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведем пример: мы хотим посчитать confidence для правила \"кто покупает пиво, тот покупает и подгузники\".\n",
    "\n",
    "Для этого сначала посчитаем, какой support у правила \"покупает пиво\", потом посчитаем support у правила \"покупает пиво и подгузники\", и просто поделим одно на другое. Т.е. мы посчитаем в скольких случаях (транзакциях) срабатывает правило \"купил пиво $supp(X)$, купил подгузники и пиво $supp(X \\cup Y)$\"\n",
    "Ничего не напоминает? Байес смотрит на все это несколько недоуменно и с презрением:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"src/images/thomas-bayes.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Lift (подъем или повышение)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Следующее понятие в нашем списке - lift. Грубо говоря, lift - это отношение \"зависимости\" items к их \"независимости\". Lift показывает, насколько items зависят друг от друга. Это очевидно из формулы:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$lift(x_1\\cup x_2) = \\frac{supp(x_1 \\cup x_2)}{supp(x_1) \\times supp(x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, мы хотим понять зависимость покупки пива и покупки подгузников. Для этого считаем support правила \"купил пиво и подгузники\" и делим его на произведение правил \"купил пиво\" и \"купил подгузники\". В случае, если lift = 1, мы говорим, что items независимы и правил совместной покупки тут нет. Если же lift > 1, то велечина, на которую lift, собственно, больше этой самой единицы, и покажет нам \"силу\" правила. Чем больше единицы, тем круче."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Conviction (убедительность)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В общем виде Conviction - это \"частотность ошибок\" нашего правила. Т.е., например, как часто покупали пиво без подгузников и наоборот."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$conv(x_1\\cup x_2) = \\frac{1 - supp(x_2)}{1 - conf(x_1 \\cup x_2)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чем результат по формуле выше ближе к 1, тем лучше. Например, если conviction покупки пива и подгузников вместе равен 1.2, это значит, что правило \"купил пиво и подгузники\" было бы в 1.2 раза (на 20%) более верным, чем если бы совпадение этих items в одной транзакции было бы чисто случайным. Немного неинтуитивное понятие, но оно и используестя н етак часто, как предыдущие три. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Используемые понятия:\n",
    "<br> -  Множество объектов (**itemset**): $X \\subseteq I = \\{x_1, x_2, ..., x_n\\}$</br>\n",
    "<br> -  Множество идентификаторов транзакций (**tidset**): $T = \\{t_1, t_2, ..., t_m\\}$</br>\n",
    "<br> -  Множество транзакций (**transactions**): $\\{(t,\\ X):\\ t\\in T,\\ X \\in I\\}$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сложность brute-force алгоритма:\n",
    "Для того чтобы найти все возможные Association rules применяя brute-force алгоритм нам необходимо перечислить все подмножества $X$ из набора $I$ и для каждого подмножества $X$ рассчитать $supp(X)$. Данный подход будет состоять из следующих шагов:\n",
    "-  генерация **кандидатов**. Данный шаг состоит из перебора всех возможных подмножеств $X$ подмножества $I$. Данные подмножества называются **кандидатами**. Поскольку каждое подмножество теоретически может являться часто встречаемым подмножеством, то множество потенциальных кандидатов будет состоять из $2^{|I|}$ элементов (здесь $|I|$ обозначается число элементов множества $I$, а $2^{|I|}$ часто называется булеаном множества $I$, то есть множество всех подмножеств $I$)\n",
    "-  расчет **support**. На данном шаге рассчитывается $supp(X)$ каждого кандидата $X$\n",
    "\n",
    "Таким образом, сложность нашего алгоритма будет: $O(|I|*|D|*2^{|I|})$, где\n",
    "<br>$O(2^{|I|})$ - количество возможных кандидатов</br>\n",
    "<br>$O(|I|*|D|)$ - сложность расчета sup(X). Поскольку для расчета  $supp(X)$ нам необходимо перебрать все элементы в $I$ в каждой транзакции $t \\in T$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Введем дополнительно еще несколько понятий.\n",
    "<br>Будем рассматривать дерево префиксов (prefix tree), где 2 элемента $X$ и $Y$ соединены, если $X$ является прямым подмножеством $Y$. Таким образом мы можем пронумеровать все подмножества множества $I$. Рисунок приведен ниже</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, рассмотрим Apriori алгоритм.\n",
    "<br>\n",
    "Apriori алгоритм использует следующее утверждение: если $X \\subseteq Y$, то $supp(X) \\geq supp(Y)$. Откуда следуют следующие 2 свойства:\n",
    " -  если $Y$ встречается часто, то любое подмножество $X: X \\subseteq Y$  так же встречается часто\n",
    " -  если $X$ встречается редко, то любое супермножество $Y: Y \\supseteq X$ так же встречается редко\n",
    " </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apriory алгоритм по-уровнево проходит по префиксному дереву и рассчитывает частоту встрчаемости подмножеств $X$ в $D$. Таким образом, в соответствии с алгоритмом:\n",
    " -  исключаются редкие подмножества и все их супемножества\n",
    " -  рассчитывается $supp(X)$ для кадого подходящего кандидата $X$ размера $k$ на уровне $k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from sklearn import... эммм... а импортировать-то и нечего:) На данный момент модулей для ALR в sklearn нет. Ну ничего, погуглим или напишем свои, правда?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "По сети гуляет целый ряд реализаций, например вот - https://github.com/asaini/Apriori, вот - http://adataanalyst.com/machine-learning/apriori-algorithm-python-3-0/, и даже вот -https://codereview.stackexchange.com/questions/38101/apriori-algorithm-using-pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы же на практике придерживаемся алгоритма apyori, написанного Ю Мочиузки (Yu Mochizuki). Полный код приводить не будем, желающие могут посмотреть тут - https://github.com/ymoch/apyori , а вот архитектуру решения и пример использования покажем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Условно решение Мочизуки можно разделить на 4 части: Структура данных, Внутренние функции, API и Прикладные функции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первая часть модуля (Структура данных) работает с изначальным датасетом. Реализуется класс TransactionManager, методы которого объединяют транзакции в матрицу, формируют список правил-кандидатов и считают support для каждого правила. Внутренние функции дополнительно по support'у формируют списки правил и соответственно их ранжируют. API логично позволяет работать напрямую с датасетами, а Прикладные функции позволяют обрабатывать транзакции и выводить результат в читаемый вид. Никакого rocketscience.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим, как использовать модуль на реальном (ну, в данном случае - игрушечном) датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# подгрузим модули\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# загрузим данные\n",
    "dataset = pd.read_csv('src/data/Market_Basket_Optimisation.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dataset.describe #всего 7500 записей-транзакций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# посомтрим на датасет\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что датасет у нас представляет разреженную матрицу, где в строках у нас набор items в каждой транзакции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#создаим из них матрицу\n",
    "transactions = []\n",
    "for i in range(0, 7501): \n",
    "    transactions.append([str(dataset.values[i,j]) for j in range(0, 20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#загружаем apriori\n",
    "import apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'apriori' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'apriori' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# и обучимся правилам. Обратите внимание, что пороговые значения мы вибираем сами в зависимости от того, /\n",
    "# насколкьо \"сильные\" правила мы хотим получит\n",
    "# min_support -- минимальный support для правил (dtype = float).\n",
    "# min_confidence -- минимальное значение confidence для правил (dtype = float)\n",
    "# min_lift -- минимальный lift (dtype = float)\n",
    "# max_length -- максимальная длина itemset (вспоминаем про k-itemset)  (dtype = integer)\n",
    "\n",
    "result = list(apriori(transactions, min_support = 0.003, min_confidence = 0.2, min_lift = 3, min_length = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# вот с визуализацией в пакете проблемы, точнее геморрой...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil, os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from StringIO import StringIO\n",
    "except ImportError:\n",
    "    from io import StringIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json #преобразовывать будем в json, используя встроенные в модуль методы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = []\n",
    "for RelationRecord in result:\n",
    "    o = StringIO()\n",
    "    apyori.dump_as_json(RelationRecord, o)\n",
    "    output.append(json.loads(o.getvalue()))\n",
    "data_df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# и взгялнем на итоги\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "display(HTML(data_df.to_html()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итого мы видим:\n",
    "\n",
    "1. Пары items\n",
    "2. items_base - первый элемент пары\n",
    "3. items_add - второй (добавленный алгоритмом) элемент пары\n",
    "4. confidence - значение confidence для пары\n",
    "5. lift - значение lift для пары\n",
    "6. support - начение support для пары. При желании, по нему можно отсортировать \n",
    "\n",
    "\n",
    "Результаты логичные: эскалоп и макароны, эскалоп и сливочно-грибной соус, курица и нежирная сметана, мягкий сыр и мед и т.д. - все это вполне логичные и, главное, вкусные сочетания:)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "ARL тот случай,скогда R-фаги могут злорадно похихикать. В R реализована библиотека arules, где присутствует и apriori, и другие алгоритмы. Официальную доку можно посмотреть тут - https://cran.r-project.org/web/packages/arules/arules.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрим на нее в действии:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала установим ее (если еще не установили):\n",
    "<br>\n",
    "install.packages('arules')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Считаем данные и преобразуем их в матрицу транзакций:\n",
    "<BR>\n",
    "<br>\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv', header = FALSE)$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на данные:\n",
    "<BR>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выучим наши правила:\n",
    "<br>\n",
    "В общем виде фнкция вызова apriori выглядит так:  $apriori(data, parameter = NULL, appearance = NULL, control = NULL)$, где\n",
    "<br>\n",
    "\n",
    "$data$ - наш датасет\n",
    "<br>\n",
    "$paramter$ - список (list) параметров для модели: минимальные support, confidence и lift\n",
    "<br>\n",
    "$appearance$ - отвечает за отображение данных. Может принимать значения lhs, rhs, both, items, none, которые определяют положение items в output\n",
    "<br>\n",
    "$control$ - отвечает за сортировку вывода (ascending, descending, без сортировки), а также за то, отображать ли прогрессбар или нет (параметр verbose)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$rules = apriori(data = dataset, parameter = list(support = 0.004, confidence = 0.2))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И посмотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'lift')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что на выходе имеем примерно те же результаты, что при использовании модуля apyori в Python:\n",
    "<br>\n",
    "\n",
    "1. {light cream}                               => {chicken}       0.004532729\n",
    "2. {pasta}                                     => {escalope}      0.005865885\n",
    "3. {pasta}                                     => {shrimp}        0.005065991\n",
    "4. {eggs,ground beef}                          => {herb & pepper} 0.004132782\n",
    "5. {whole wheat pasta}                         => {olive oil}     0.007998933\n",
    "6. {herb & pepper,spaghetti}                   => {ground beef}   0.006399147\n",
    "7. {herb & pepper,mineral water}               => {ground beef}   0.006665778\n",
    "8. {tomato sauce}                              => {ground beef}   0.005332622\n",
    "9. {mushroom cream sauce}                      => {escalope}      0.005732569\n",
    "10. {frozen vegetables,mineral water,spaghetti} => {ground beef}   0.004399413"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, в R apriori использовать на данный момент намного удобнее, чем в Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ECLAT Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Идея алгоритма ECLAT (Equivalence CLAss Transformation) заключается в ускорении подсчета $supp(X)$. Для этого нам необходимо проиндексировать наше базу данных $D$ так, чтобы это позволило быстро рассчитывать $supp(X)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Легко заметить, что если $t(X)$ обозначает множество всех транзакций, где встречается подмножество $X$, то\n",
    "<br>$t(XY) = t(X) \\cap t(Y)$</br>\n",
    "<br>и</br>\n",
    "<br>$supp(XY) = |t(XY)|$</br>\n",
    "<br> то есть $supp(XY)$ равен кардинальности (размеру) множества $t(XY)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"/src/images/tree_3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный подход может быть значительно усовершенствован путем уменьшения размера промежуточных множеств идентификаторов транзакций (tidsets). А именно, мы можем хранить не все множество транзакций на промежуточном уровне, а только множество различий этих транзакций. Предположим, что\n",
    "<br>$X_a = \\{x_1, x_2,..., x_{n-1}, a\\}$</br>\n",
    "<br>$X_b = \\{x_1, x_2,..., x_{n-1}, b\\}$</br>\n",
    "<br>Тогда, мы получим: </br>\n",
    "<br>$X_{ab} = \\{x_1, x_2,..., x_{n-1}, a, b\\}$</br>\n",
    "<br>$diffset(X_{ab})$ это множество всех id транзакций, которыесодержат префикс $X_a$ но не содержат элемент $b$: </br>\n",
    "<br>$d(X_{ab}) =t(X_a)/t(X_{ab})=t(X_a)/t(X_{b})$</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"/src/images/tree_4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "И вновь пользователи R ликуют, для них никаких танцев с бубном делать не надо, все по аналогии с apriori. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Запускаем библиотеку и читаем данные:\n",
    "\n",
    "$library(arules)$\n",
    "<br>\n",
    "$dataset = read.csv('Market_Basket_Optimisation.csv')$\n",
    "<br>\n",
    "$dataset = read.transactions('Market_Basket_Optimisation.csv', sep = ',', rm.duplicates = TRUE)$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Быстрый взгляд на датасет:\n",
    "<br>\n",
    "$summary(dataset)$\n",
    "<br>\n",
    "$itemFrequencyPlot(dataset, topN = 10)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Правила:\n",
    "<br>\n",
    "$rules = eclat(data = dataset, parameter = list(support = 0.003, minlen = 2))$ \n",
    "<br>\n",
    "Обратите внимание, настраиваем толкьо support и минимальную длину (k в k-itemset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И смотрим на результаты:\n",
    "<br>\n",
    "$inspect(sort(rules, by = 'support')[1:10])$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FP-Growth Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Теория"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализации FP-Growth в Питоне повезло не больше, чем другим ALR-алгоритмам. Стандартных библиотек под него нет."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Неплохо FP_Growth представлен в pyspark, смотреть тут - http://spark.apache.org/docs/2.2.0/mllib-frequent-pattern-mining.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На gitHub тоже можно найти несколкьо решений эпохи неолита, например тут - https://github.com/enaeseth/python-fp-growth и тут - https://github.com/evandempsey/fp-growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Потестим второй вариант"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyfpgrowth\n",
      "  Downloading pyfpgrowth-1.0.tar.gz (1.6MB)\n",
      "Building wheels for collected packages: pyfpgrowth\n",
      "  Running setup.py bdist_wheel for pyfpgrowth: started\n",
      "  Running setup.py bdist_wheel for pyfpgrowth: finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\pavel.golubev\\AppData\\Local\\pip\\Cache\\wheels\\b0\\77\\ee\\b8139db041c69f769d35de3373fe03129c25ef186521e924f1\n",
      "Successfully built pyfpgrowth\n",
      "Installing collected packages: pyfpgrowth\n",
      "Successfully installed pyfpgrowth-1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyfpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyfpgrowth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 29.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "patterns = pyfpgrowth.find_frequent_patterns(transactions, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rules = pyfpgrowth.generate_association_rules(patterns, 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Реализация в R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае R не отстает от Питона: в такой удобнйо и родной arules FP-Growth отсутствует. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "В то же время, как и для Питона, реализация сущетсвует в Spark - https://spark.apache.org/docs/2.2.0/api/R/spark.fpGrowth.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Заключение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Итак, мы познакомились с базовой теорией ARL (\"кто купил х, также купил y\") и основными понятиями и метриками (support, confidence, lift и conviction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Посмотрели 3 самых популярных алгоритма (Apriori, ECLAT, FP-Growth), позавидовали пользователям R и библиотеки arules, попробовали сами реализовать ECALT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что в итоге:\n",
    "1. ARL лежат в основе рекомендательных систем\n",
    "2. ARL широко применимы - от традиционного ритейла и онлайн ритейла (от Ozon до Steam) до банков и телекома (подключаемые сервисы и услуги)\n",
    "3. ARL относительно легко использовать, существуют реализации разного уровня проработки для разных задач.\n",
    "4. ARL хорошо интепретируются и не требуют специальных навыков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В заключении не можем не упомянуть о сумрачном гении ARL докторе Кристиане Боргельте из Университета Констанца (http://www.borgelt.net/software.html).\n",
    "На самом деле все, с чем мы мучались на протяжении этой статьи им давно уже реализовано на С, Python, Java и R. Ну или почти все. Существет даже GUI за его авторством, где в пару кликов можно загрузить датасет, выбрать нужный алгоритм и найти правила. Это при условии, что оно у вас заработает:)\n",
    "Для простых же задач достаточно и того, что мы рассмотрели в этой статье. А если недостаточно - призываем писать реализацию самим! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использованная литература:\n",
    "<br>\n",
    "[1] Discovery, analysis and presentation of strong rules. G. Piatetsky-Shapiro. Knowledge Discovery in Databases, AAAI Press, (1991)\n",
    "<br>\n",
    "[2] Mining Association Rules between Sets of Items in Large Databases http://arbor.ee.ntu.edu.tw/~chyun/dmpaper/agrama93.pdf\n",
    "<br>\n",
    "[3] Fast Algorithms for Mining Association Rules http://www.vldb.org/conf/1994/P487.PDF\n",
    "<br>\n",
    "[4] Ask Dan! http://www.dssresources.com/newsletters/66.php\n",
    "<br>\n",
    "[5] Introduction to arules – A computational environment for mining association rules and frequent item sets http://www.lsi.upc.edu/~belanche/Docencia/mineria/Practiques/R/arules.pdf\n",
    "<br>\n",
    "[6] Публикации Д-ра Боргельта - http://www.borgelt.net/publications.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
